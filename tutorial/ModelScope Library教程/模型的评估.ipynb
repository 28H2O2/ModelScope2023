{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 指标的意义\n",
        "\n",
        "指标用来衡量模型对数据集的效果好坏。例如Accuracy、Precision、Recall、F1、Rouge等。单一指标一般无法直接用于某类任务的评估上，因为一类任务的评价指标\n",
        "可能比较复杂，比如token-classification任务的评价指标融合了Accuracy、Precision、Recall、F1四种单一指标。为了方便用户针对某类任务直接进行评测，ModelScope\n",
        "提供了Metric任务级别的高级封装，其内部可能会调用开源框架进行单一指标评价。\n",
        "\n",
        "# Metric模块\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "class Metric(ABC):\n",
        "    \"\"\"The metric base class for computing metrics.\n",
        "\n",
        "    The subclasses can either compute a single metric like 'accuracy', or compute the\n",
        "    complex metrics for a specific task with or without other Metric subclasses.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def add(self, outputs: Dict, inputs: Dict):\n",
        "        \"\"\" Append logits and labels within an eval loop.\n",
        "\n",
        "        Will be called after every batch finished to gather the model predictions and the labels.\n",
        "\n",
        "        Args:\n",
        "            outputs: The model prediction outputs.\n",
        "            inputs: The mini batch inputs from the dataloader.\n",
        "\n",
        "        Returns: None\n",
        "\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def evaluate(self):\n",
        "        \"\"\"Evaluate the metrics after the eval finished.\n",
        "\n",
        "        Will be called after the whole validation finished.\n",
        "\n",
        "        Returns: The actual metric dict with standard names.\n",
        "\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metric的基类定义非常简单，仅有add和evaluate两个方法。add方法用于收集每一个mini-batch的模型输入和输出信息，并从中遴选出需要用于计算指标的信息，\n",
        "比如inputs中的labels字段，以及outputs中的logits字段。add方法会在每次mini-batch执行完调用，且不存在多线程问题。\n",
        "\n",
        "evaluate方法会在整个数据集跑完之后执行，它没有输入参数，evaluate内部会对add收集的数值进行汇总和计算，并给出Dict形式的指标输出。\n",
        "\n",
        "# ModelScope支持的Metric类\n",
        "\n",
        "## NLP\n",
        "\n",
        "### sequence_classification_metric\n",
        "#### 方法说明\n",
        "\n",
        "用于评估文本分类任务的结果，这个Metric会在每个mini-batch跑完后收集模型产出的logits字段和labels字段，在整体数据集评估结束时产出Accuracy指标的值，并以dict形式返回。\n",
        "#### 返回格式：\n",
        "\n",
        "```json\n",
        "{\"accuracy\": 0.90}\n",
        "```\n",
        "\n",
        "sequence_classification_metric可以用于各类模型的文本分类任务。使用场景要求为：\n",
        "\n",
        "- 模型输出是dict-like结构，并具有`logits`字段，其最后一维和标签数量相等\n",
        "- 模型输入有内容为标签id的labels或label字段\n",
        "- 任务类型为`单标签`的文本分类任务\n",
        "\n",
        "我们为NLP部分任务类型指定了默认metric类别：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.metrics.builder import task_default_metrics\n",
        "from modelscope.utils.constant import Tasks\n",
        "print(task_default_metrics[Tasks.sentence_similarity])\n",
        "print(task_default_metrics[Tasks.nli])\n",
        "print(task_default_metrics[Tasks.sentiment_classification])\n",
        "print(task_default_metrics[Tasks.text_classification])\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "nli/sentiment-classification/sentence-similarity/text_classification任务的训练过程会默认使用这个metric进行测试。\n",
        "\n",
        "用户也可以在cfg中指定自己需要的指标类型。如果进行指定，会覆盖task_default_metrics中的默认metric：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "def cfg_modify_fn(cfg):\n",
        "    cfg.task = 'text-classification'\n",
        "    cfg.preprocessor.first_sequence = 'sentence1'\n",
        "    cfg.preprocessor.second_sequence = 'sentence2'\n",
        "    cfg.preprocessor.label = 'label'\n",
        "    cfg.preprocessor.label2id = {'0': 0, '1': 1}\n",
        "    # 指定指标类型\n",
        "    cfg.evaluation.metrics = 'seq-cls-metric'\n",
        "    return cfg\n",
        "\n",
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.trainers import build_trainer\n",
        "dataset = MsDataset.load('clue', subset_name='afqmc')\n",
        "kwargs = dict(\n",
        "    model='damo/nlp_structbert_backbone_base_std',\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    work_dir='/tmp',\n",
        "    cfg_modify_fn=cfg_modify_fn)\n",
        "\n",
        "trainer = build_trainer(name='nlp-base-trainer', default_args=kwargs)\n",
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### token_classification_metric\n",
        "\n",
        "#### 方法说明\n",
        "\n",
        "用于评估序列标注任务的结果，会在每个mini-batch跑完后收集模型产出的logits字段和labels字段，在整体数据集评估结束时产出precision、recall、F1、accuracy四个指标的值，并以dict形式返回。\n",
        "该Metric使用了seqeval作为内部计算实现。\n",
        "#### 返回格式：\n",
        "\n",
        "token-classification的label一般是BIO、BIOES等标注方式。如果需要查看有关tagging的一些基础信息，可以点击[这里](https://zhuanlan.zhihu.com/p/147537898)。\n",
        "\n",
        "假设原始label为B-obj,I-obj,O，即只存在obj和O两种token类型, 那么metric的返回值类似于这样：\n",
        "```json\n",
        "{\n",
        "\"obj\": \n",
        " {\"precision\": 0.25, \n",
        "  \"recall\": 0.5, \n",
        "  \"f1\": 0.3333333333333333, \n",
        "  \"number\": 2\n",
        " }, \n",
        " \"precision\": 0.25, \n",
        " \"recall\": 0.5, \n",
        " \"f1\": 0.3333333333333333, \n",
        " \"accuracy\": 0.5\n",
        "}\n",
        "```\n",
        "是否返回每种label（如上代码中的obj）的详细指标可以通过构造方法中的return_entity_level_metrics参数控制，该参数默认为False，即关闭状态。\n",
        "\n",
        "token_classification_metric可以用于各类模型的序列标注任务。使用场景要求为：\n",
        "- 模型输出是dict-like结构，并具有'logits'字段，其最后一维和标签数量相等\n",
        "- 模型输入有内容为标签id的labels或label字段\n",
        "\n",
        "token-classification任务的训练过程可以在cfg中不指定而直接使用这个metric，同sequence_classification_metric一样，用户也可以在cfg中指定自己需要的指标类型。\n",
        "\n",
        "### text_generation_metric\n",
        "#### 方法说明\n",
        "\n",
        "用于评估文本生成任务的结果，这个Metric会在每个mini-batch跑完后收集模型产出的预测文本和目标文本，在整体数据集评估结束时产出 rouge-l F1 指标的值，并以dict形式返回。\n",
        "该Metric使用了 rouge_score 作为内部计算实现。\n",
        "\n",
        "#### 返回格式：\n",
        "```json\n",
        "{\"f1\": 0.35}\n",
        "```\n",
        "\n",
        "text_generation_metric 可以用于各类模型的文本生成任务。使用场景要求为：\n",
        "- 模型输出是dict-like结构，并具有 preds 字段（类型为 List[str]，为预测文本列表，与 tgts 等长）\n",
        "- 模型输入有 tgts 字段（类型为 List[str]，为目标文本列表）\n",
        "用户也可以在cfg中指定自己需要的指标类型：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# 指定使用SequenceClassificationMetric\n",
        "cfg.evaluation.metrics = 'text-gen-metric'\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## CV\n",
        "### image_instance_segmentation_metric\n",
        "#### 方法说明\n",
        "\n",
        "用于评估常见的two-stage、COCO数据集格式的实例分割模型，名称为`image-ins-seg-coco-metric`，使用标准的cocoapi进行评估，输出指标包括如下：\n",
        "```json\n",
        "{\n",
        "    \"bbox_mAP\": float, \n",
        "    \"bbox_mAP_50\": float, \n",
        "    \"bbox_mAP_75\": float, \n",
        "    \"bbox_mAP_s\": float, \n",
        "    \"bbox_mAP_m\": float, \n",
        "    \"bbox_mAP_l\": float, \n",
        "    \"segm_mAP\": float,\n",
        "    \"segm_mAP_50\": float, \n",
        "    \"segm_mAP_75\": float, \n",
        "    \"segm_mAP_s\": float, \n",
        "    \"segm_mAP_m\": float, \n",
        "    \"segm_mAP_l\": float\n",
        "}\n",
        "```\n",
        "\n",
        "模型的输出output是一个字典，包含关键字 'eval_result' 和 `img_metas`，其中 output['img_metas']\n",
        "包含数据集标注文件路径参数`ann_file`和数据集类别名称`classes`信息，output['eval_result']为模型推理输出，格式为 list[tuple]，\n",
        "它包含预测的 bbox 结果和 mask 结果。外部列表对应于每个图像，内部元组第一个元素是 bbox 结果，第二个元素是 mask 结果。\n",
        "具体代码可见modelscope/metrics/image_instance_segmentation_metric.py。\n",
        "\n",
        "\n",
        "# 在Trainer中使用Metrics\n",
        "\n",
        "metrics会在modelscope训练过程的交叉验证中，或单独的测试流程中被调用。\n",
        "\n",
        "训练过程和验证流程的文档可以查看[这里]()。用户只需要在cfg中指定好需要的metrics，或使用任务默认的metrics，验证过程会自动使用这些类。\n",
        "\n",
        "# 在外部训练中使用Metrics\n",
        "\n",
        "您可以在外部框架的训练中单独使用这些类。下面我们以transformers的Trainer为例来使用文本分类的Metric。\n",
        "\n",
        "首先构造模型、数据集、tokenizer并预处理数据集：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('clue', 'afqmc')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "dataset = dataset.map(tokenize_function, batched=True)\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(64))\n",
        "small_eval_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(64))\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "构造Trainer并传入Metric类：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from modelscope.metrics.sequence_classification_metric import SequenceClassificationMetric\n",
        "training_args = TrainingArguments(output_dir=\"/tmp\", evaluation_strategy='steps', \n",
        "                                  metric_for_best_model='accuracy', eval_steps=1)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    metric = SequenceClassificationMetric()\n",
        "    metric.add({'logits': logits}, {'labels': labels})\n",
        "    return metric.evaluate()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 如何编写新的Metric类\n",
        "\n",
        "Metric类比较简单，您只需要实现add和evaluate两个方法即可。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "class MyCustomMetric:\n",
        "\n",
        "    def add(self, outputs, inputs):\n",
        "        # TODO outputs是模型输出，inputs为模型输入，在这里取出所需要的value并存下来\n",
        "        pass\n",
        "\n",
        "    def evaluate(self):\n",
        "        # TODO 在数据及验证完成是调用，根据add存下来的value进行计算\n",
        "        pass\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "在存在数据并行的情况下，add传入的值需要先经过allgather过程，在rank0上传入Metric，这部分能力一般在Trainer中体现，使用时需要注意。\n",
        "\n",
        "如果需要将这个Metric注册到modelscope中，只需要手动调用：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.utils.registry import default_group\n",
        "from modelscope.metrics import METRICS\n",
        "METRICS.register_module(group_key=default_group, module_name='my-custom-metric', module_cls=MyCustomMetric)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "使用的时候指定cfg.evaluation.metrics = 'my-custom-metric'即可。\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
