{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# README.ipynb文件\n",
        "### 作用\n",
        "README.ipynb文件主要用来描述数据集信息，由YAML元数据和Markdown文档内容组成，最终内容可通过数据集介绍标签页面进行查看。为了社区用户更好地了解和发现您的数据集，建议您认真维护此文件。您可以通过在页面上点击 README.ipynb右侧的编辑按钮，进入到编辑页以后，使用右上角的 “使用模板编辑器”来帮助您创建更为规范准确的自述文件。![image.png](./resources/1661138137815-86eabd4c-7c2a-40bb-8a39-b18c337601bb.png)\n",
        "### 组成\n",
        "我们推荐数据集卡片提供如下内容描述，可通过数据集介绍页面进行查看，包括但不限于：\n",
        "\n",
        "- **数据集协议、标签信息**。其中标签包含系统推荐标签和用户自定义标签。\n",
        "- **数据集描述与简介**。介绍该数据集的基础信息、使用场景、子数据集、用途、数据量等。\n",
        "- **支持的模型**。明确该数据集支持的模型信息。\n",
        "- **数据集的格式和结构**。 包含数据的schema信息，并提供必要的数据样本示范。\n",
        "- **如何使用**。 可以给出简单示例介绍用户如何使用该数据集，包括所使用的框架、运行环境要求等。若给出代码范例供效果更佳。\n",
        "- **数据集生成的相关信息**。可以包含原始数据来源、数据标注方式、标注过程等。\n",
        "### 官方样例\n",
        "一个有效的数据集卡片需要包含YAML头部信息和Markdown文本。 头部的YAML信息使用---分组进行区隔。一份完整的YAML部分的内容参考如下：\n",
        "```\n",
        "---\n",
        "license: Apache License 2.0\n",
        "#用户自定义标签\n",
        "tags:\n",
        "- Alibaba\n",
        "- arxiv:1810.99999\n",
        "- my free-style tag\n",
        "\n",
        "text:\n",
        "  #二级只能属于一个task_categories\n",
        "  fill_mask:\n",
        "    #三级可以多选\n",
        "    languages:\n",
        "    - en\n",
        "    multilinguality:\n",
        "    - monolingual\n",
        "    \n",
        "audio:\n",
        "  automatic_speech_recognition:\n",
        "    languages:\n",
        "    - en\n",
        "    - fr\n",
        "    sampling_rates:\n",
        "    - 16000 \u003c!--- integer ---\u003e\n",
        "    - 64000\n",
        "\n",
        "image:\n",
        "  Image-to-Text:\n",
        "    resolutions:\n",
        "      - 640 x 480 \n",
        "      - 1024 x 720\n",
        "    color_space:\n",
        "    - rgb\n",
        "    encoding:\n",
        "    - jpeg\n",
        "\n",
        "video:\n",
        "  Object-Detection:\n",
        "    resolutions:\n",
        "    - 640 x 480\n",
        "    - 1024 x 720\n",
        "    encoding:\n",
        "    - mpeg\n",
        "   \n",
        "multi_modal:\n",
        "    Feature Extraction:\n",
        "      resolutions:\n",
        "      - 640 x 480\n",
        "      encoding:\n",
        "      - H264\n",
        "      languages:\n",
        "      - en\n",
        "      multilinguality:\n",
        "      - monolingual\n",
        "\n",
        "    \n",
        "---    \n",
        "\u003c!--- 以上YAML section提供属性/tags描述---\u003e\n",
        "\n",
        "\u003c!--- 以下为markdown格式的dataset描述---\u003e\n",
        "\n",
        "## 数据集描述\n",
        "数据集整体描述。\n",
        "\n",
        "### 数据集简介\n",
        "提供对于数据集的介绍，支持的使用场景（包括支持的语言等）。\n",
        "\n",
        "### 数据集支持的任务\n",
        "该数据集支持的训练任务，以及相关benchmark结果。\n",
        "\n",
        "\n",
        "## 数据集的格式和结构\n",
        "\n",
        "### 数据格式\n",
        "对数据的格式进行描述，包括数据的schema，以及提供必要的数据样本示范。\n",
        "如果数据集内含多个子数据集的话，每个字数据集都应该提供相对应的数据格式描述。\n",
        "\n",
        "\n",
        "### 数据集加载方式\n",
        "通过代码范例等方式，提供数据集通过MaaS/Dataset SDK进行加载和使用的详细说明。\n",
        "\n",
        "### 数据分片\n",
        "数据集是否进行了预分片（例如是否有预设的train/test/validation的数据分片）。\n",
        "如果有，数据的分片时如何实现的。\n",
        "如果没有预先分片，是否对于数据使用过程中的分片有什么推荐（比例等）。\n",
        "\n",
        "\n",
        "\n",
        "## 数据集生成的相关信息\n",
        "\n",
        "### 原始数据\n",
        "描述原始数据的来源以及数据的初步收集是如何进行的，是否经过归一化等处理流程。\n",
        "\n",
        "### 数据集标注\n",
        "该数据集是否包含标注，若有的话，相关信息描述。\n",
        "\n",
        "#### 标注过程\n",
        "标注是通过什么方式实现的，流程如何。\n",
        "\n",
        "#### 标注者\n",
        "标注者相关信息，尤其是当标着和原始数据提供者有所区别时。\n",
        "\n",
        "\n",
        "\n",
        "## 数据集版权信息\n",
        "\n",
        "数据集相关的版权信息，授权使用的场景和用户。是否开源，以及采用哪个开源协议等等。\n",
        "\n",
        "## 引用方式\n",
        "\n",
        "数据集是否有相关联的文章，以及如果在研究论文中要引用该数据集是否有推荐的引用格式等等。\n",
        "\n",
        "## 其他相关信息\n",
        "\n",
        "该数据集可能包含的个人和敏感信息，使用数据集需要考虑的相关背景；\n",
        "数据集可能包含的社会意义以及其中可能包含的bias信息和可能的局限性等等。\n",
        "```\n",
        "# 托管到ModelScope数据集文件规范\n",
        "## CSV文件\n",
        "### 作用\n",
        "当您选择了创建一个“托管到ModelScope”的数据集时，CSV文件是用于描述您的数据文件（zip）里的数据和其对应的meta是如何组织的。\n",
        "### 组成\n",
        "**首行**为header， 采用 逗号分隔， 每个字段 采用   {字段名}:{类型} 作为标识， {类型} 可以没有，则默认文本（Value）, 字段名不允许有重复，**_字段名不要以英文下划线\"_\"开头，可能会与系统保留字段冲突_**。\n",
        "**第二行开始**为具体的数据，依然采用逗号分隔，字段数和header保持一致，涉及到FILE类型的时候，请填写其相对路径。\n",
        "支持的**{类型}** 枚举\n",
        "\n",
        "- FILE    ：  媒资文件相对zip包的路径 （会根据后缀解析为Image， Video， Audio）\n",
        "### 官方样例\n",
        "```basic\n",
        "Title,Description,Input Video:FILE,Generated Video:FILE,Category:LABEL\n",
        "男士衣服,宽松连体,train/input/0001.mp4,train/output/0001.mp4,服装\n",
        "儿童玩具,五彩缤纷,train/input/0002.mp4,train/output/0002.mp4,玩具\n",
        "```\n",
        "\n",
        "CSV Example ： \n",
        "\n",
        "- [demo_test.csv ](https://dataset-hub.oss-cn-hangzhou.aliyuncs.com/documentation/example/demo_test.csv)\n",
        "- [demo_train.csv](https://dataset-hub.oss-cn-hangzhou.aliyuncs.com/documentation/example/demo_train.csv)\n",
        "- [pictures.zip](https://dataset-hub.oss-cn-hangzhou.aliyuncs.com/documentation/example/pictures.zip)\n",
        "## 同名JSON文件\n",
        "### 作用\n",
        "当您选择了创建一个“托管到ModelScope”的数据集时，JSON文件是用于描述多个CSV文件和多个数据文件是如何互相映射的，以及每对映射是用于哪个子数据集或用途。\n",
        "\n",
        "### 组成\n",
        "如下：\n",
        "外层key， 记录 子数据集的名称，默认可以填写为“default”；\n",
        "每个子数据集内层的key，记录用途名称，默认填写为“train” （可以从train ，test， validate 选择）；\n",
        "最内层键值对 ，记录 CSV文件（Meta字段）到 ZIP数据文件（file字段）的映射关系。\n",
        "**同一个Subset下不同的CSV文件，请保证Schema 一致，否则会报错。**\n",
        "```sql\n",
        "{\n",
        "\t\"default\": {\n",
        "\t\t\"train\": {\n",
        "\t\t\t\"meta\": \"my_train.csv\",\n",
        "\t\t\t\"file\": \"pictures.zip\"\n",
        "\t\t}\n",
        "\t},\n",
        "\t\"subsetA\": {\n",
        "\t\t\"test\": {\n",
        "\t\t\t\"meta\": \"mytest.csv\",\n",
        "\t\t\t\"file\": \"pictures.zip\"\n",
        "\t\t}\n",
        "\t},\n",
        "\t\"videoSubset\": {},\n",
        "\t\"kangdiSubset\": {\n",
        "\t\t\"test\": {\n",
        "\t\t\t\"meta\": \"test_dataset.csv\",\n",
        "\t\t\t\"file\": \"pictures.zip\"\n",
        "\t\t},\n",
        "\t\t\"train\": {\n",
        "\t\t\t\"meta\": \"test_dataset.csv\",\n",
        "\t\t\t\"file\": \"pictures.zip\"\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "```\n",
        "创建一个“托管到ModelScope”的数据集会默认生成一个同名json文件。\n",
        "\n",
        "在前端通过交互以后生成并更新，您当然也可以按照规范在，自行在本地编辑好上传：\n",
        "![image.png](./resources/1659343035958-a9be78e7-efda-48de-941e-61800e88ee2f.png)\n",
        "![image.png](./resources/1659343020769-f0e25a8a-5c90-4eb0-902a-d49dd3be7d86.png)\n",
        "\n",
        "### 官方样例\n",
        "```sql\n",
        "{\n",
        "\t\"default\": {\n",
        "\t\t\"train\": {\n",
        "\t\t\t\"meta\": \"demo_train.csv\",\n",
        "\t\t\t\"file\": \"pictures.zip\"\n",
        "\t\t}\n",
        "\t},\n",
        "\t\"subsetA\": {\n",
        "\t\t\"test\": {\n",
        "\t\t\t\"meta\": \"demo_test.csv\",\n",
        "\t\t\t\"file\": \"pictures.zip\"\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "```\n",
        "# \n",
        "\n",
        "## ZIP文件\n",
        "### 作用\n",
        "ZIP文件是您的真实媒资数据所在，可以通过前端上传：![image.png](./resources/1659343311875-d1602dbb-9ec8-4451-8758-edb232c76899.png)\n",
        "目前小于5G的压缩包，我们都会为您解压缩，并根据CSV文件和同名JSON文件，为您提供数据预览功能，大于5G则不会进行解压和预览，只作为数据托管用途。\n",
        "多文件\n",
        "# 托管到公开Host数据集文件规范\n",
        "## 同名Python文件\n",
        "### 作用\n",
        "如果您的数据已经托管在公开的Host上，在创建数据集时候，选择“托管到公开Host”，我们会为您自动生成一个数据集同名Python文件，Python文件是为了告诉ModelScope如何可以正确解析您提供的数据集，数据是怎样组织的。\n",
        "### 组成\n",
        "同名的py文件下有一个同名的Python class，继承了datasets.GeneratorBasedBuilder 类，有三个方法需要您来实现：\n",
        "\n",
        "- _info  方法：主要是对数据集的基本描述，一些外链的信息组织；**_字段名不要以英文下划线\"_\"开头，可能会与系统保留字段冲突_**。\n",
        "- _split_generators： 主要是告诉我们如何下载这个外链的数据集，并将数据集的组成根据用途进行划分，e.g. trian, split 或者 validate；\n",
        "- _generate_examples : 主要是告诉我们每条数据是如何组织的，下载下来的数据如何可以被转换成一个便于被训练加载的记录；\n",
        "### 官方样例\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "# Lint as: python3\n",
        "\"\"\"FashionMNIST Data Set\"\"\"\n",
        "\n",
        "\n",
        "import struct\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import datasets\n",
        "from datasets.tasks import ImageClassification\n",
        "\n",
        "\n",
        "_CITATION = \"\"\"\\\n",
        "@article{DBLP:journals/corr/abs-1708-07747,\n",
        "  author    = {Han Xiao and\n",
        "               Kashif Rasul and\n",
        "               Roland Vollgraf},\n",
        "  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n",
        "               Algorithms},\n",
        "  journal   = {CoRR},\n",
        "  volume    = {abs/1708.07747},\n",
        "  year      = {2017},\n",
        "  url       = {http://arxiv.org/abs/1708.07747},\n",
        "  archivePrefix = {arXiv},\n",
        "  eprint    = {1708.07747},\n",
        "  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\n",
        "  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\n",
        "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "_DESCRIPTION = \"\"\"\\\n",
        "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of\n",
        "60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\n",
        "associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\n",
        "replacement for the original MNIST dataset for benchmarking machine learning algorithms.\n",
        "It shares the same image size and structure of training and testing splits.\n",
        "\"\"\"\n",
        "\n",
        "_HOMEPAGE = \"https://github.com/zalandoresearch/fashion-mnist\"\n",
        "_LICENSE = \"https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/LICENSE\"\n",
        "\n",
        "_URL = \"http://vpf-pre.oss-cn-hangzhou.aliyuncs.com/tmp/dataset/fashion/\"\n",
        "_URLS = {\n",
        "    \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
        "    \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
        "    \"test_images\": \"t10k-images-idx3-ubyte.gz\",\n",
        "    \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",\n",
        "}\n",
        "\n",
        "_NAMES = [\n",
        "    \"T - shirt / top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "\n",
        "class FashionMnist(datasets.GeneratorBasedBuilder):\n",
        "    \"\"\"FashionMNIST Data Set\"\"\"\n",
        "\n",
        "    BUILDER_CONFIGS = [\n",
        "        datasets.BuilderConfig(\n",
        "            name=\"fashion_mnist\",\n",
        "            version=datasets.Version(\"1.0.0\"),\n",
        "            description=_DESCRIPTION,\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    def _info(self):\n",
        "        return datasets.DatasetInfo(\n",
        "            description=_DESCRIPTION,\n",
        "            features=datasets.Features(\n",
        "                {\n",
        "                    \"image\": datasets.Image(),\n",
        "                    \"label\": datasets.features.ClassLabel(names=_NAMES),\n",
        "                }\n",
        "            ),\n",
        "            supervised_keys=(\"image\", \"label\"),\n",
        "            homepage=_HOMEPAGE,\n",
        "            citation=_CITATION,\n",
        "            task_templates=[ImageClassification(image_column=\"image\", label_column=\"label\")],\n",
        "        )\n",
        "\n",
        "    def _split_generators(self, dl_manager):\n",
        "        urls_to_download = {key: _URL + fname for key, fname in _URLS.items()}\n",
        "        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n",
        "\n",
        "        return [\n",
        "            datasets.SplitGenerator(\n",
        "                name=datasets.Split.TRAIN,\n",
        "                gen_kwargs={\n",
        "                    \"filepath\": [downloaded_files[\"train_images\"], downloaded_files[\"train_labels\"]],\n",
        "                    \"split\": \"train\",\n",
        "                },\n",
        "            ),\n",
        "            datasets.SplitGenerator(\n",
        "                name=datasets.Split.TEST,\n",
        "                gen_kwargs={\n",
        "                    \"filepath\": [downloaded_files[\"test_images\"], downloaded_files[\"test_labels\"]],\n",
        "                    \"split\": \"test\",\n",
        "                },\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "    def _generate_examples(self, filepath, split):\n",
        "        \"\"\"This function returns the examples in the raw form.\"\"\"\n",
        "        # Images\n",
        "        with open(filepath[0], \"rb\") as f:\n",
        "            # First 16 bytes contain some metadata\n",
        "            _ = f.read(4)\n",
        "            size = struct.unpack(\"\u003eI\", f.read(4))[0]\n",
        "            _ = f.read(8)\n",
        "            images = np.frombuffer(f.read(), dtype=np.uint8).reshape(size, 28, 28)\n",
        "\n",
        "        # Labels\n",
        "        with open(filepath[1], \"rb\") as f:\n",
        "            # First 8 bytes contain some metadata\n",
        "            _ = f.read(8)\n",
        "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "\n",
        "        for idx in range(size):\n",
        "            yield idx, {\"image\": images[idx], \"label\": int(labels[idx])}\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "45-51行的URL信息，就是您应该提供的URL信息，该信息，在94-95 行被下载和解压。\n",
        "\n",
        "值得注意的是，许多公开数据集的文件地址在海外，平时难以下载，或者下载很慢，笔者这里预先将fashion_mnist的数据集zip包手动下载并放到了自己的oss上面，并设置为公共读权限。\n",
        "\n",
        "建议在本地编写好Python文件以后，先验证一下:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "\u003e\u003e from datasets import load_dataset\n",
        "\u003e\u003e fashion_mnist = load_dataset('./fashion_mnist.py')\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "如果可以成功load 到数据集，则至少说明python文件是符合要求的。![image.png](./resources/1656324519707-cfa0660e-a4d6-4494-8422-02e9a34f39c5.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "\u003e\u003e\u003e fashion_mnist\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['image', 'label'],\n",
        "        num_rows: 60000\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['image', 'label'],\n",
        "        num_rows: 10000\n",
        "    })\n",
        "})\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ZIP文件\n",
        "### 作用\n",
        "同【托管到ModelScope数据集文件规范】-\u003e【ZIP文件】。\n",
        "# \n",
        "### \n",
        "\n",
        "# \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
