{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 数据预处理的概念\n",
        "\n",
        "在深度学习中，模型的输入并不是实际的文字、图片或音频的原始信息。以NLP来说，文字或单词可能会先经过拆分编码转换为字元，再经过一个字典（vocab）转换为一个数字编号，\n",
        "举个例子，reading可能会被转换为#read和#ing，而后变为字典编号1和19（仅作为例子，其中的数字和字符可能不准确）。以CV来说，图片在输入模型之前可能会经过增强、旋转、裁剪、二值化等过程，形成指定大小和规则的新图片，\n",
        "这个过程就是预处理。\n",
        "\n",
        "# ModelScope的预处理器\n",
        "\n",
        "预处理器在ModelScope中的作用比单纯的tokenizer更宽泛，它开始于数据从数据集中取出，终止于处理好的数据输入模型。因此预处理过程会关心数据集本身特性，比如会在构造方法中传入数据集的key用来获取对应字段的值。\n",
        "\n",
        "ModelScope的预处理器也是通过注册被自动使用的，它的注册方式是领域+预处理器名字，例如：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# 注册一个NLP领域的fill-mask预处理器\n",
        "@PREPROCESSORS.register_module(Fields.nlp, module_name=Preprocessors.fill_mask)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "如果是外部的预处理器，也可以通过直接调用的方法进行注册：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "PREPROCESSORS.register_module(Fields.nlp, module_name=Preprocessors.fill_mask, module_cls=MyPreprocessorCls)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "预处理器可以这样使用：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.hub.snapshot_download import snapshot_download\n",
        "from modelscope.preprocessors import SequenceClassificationPreprocessor\n",
        "from modelscope.models.nlp import SbertForSequenceClassification\n",
        "model_dir = snapshot_download('damo/nlp_structbert_sentence-similarity_chinese-base')\n",
        "# 直接构造，这时候注册机制不起作用\n",
        "preprocessor = SequenceClassificationPreprocessor(model_dir=model_dir, sequence_length=256)\n",
        "model = SbertForSequenceClassification.from_pretrained(model_dir)\n",
        "#输入一个tuple\n",
        "data = preprocessor(('这件商品很好', '这件商品很优秀'))\n",
        "print(data)\n",
        "print(model(**data)) # AttentionTextClassificationModelOutput(logits=tensor([[-1.3232,  1.5160]], grad_fn=\u003cAddmmBackward0\u003e), loss=None, attentions=None, hidden_states=None)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "也可以调用Preprocessor的from_pretrained方法：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.preprocessors import Preprocessor\n",
        "from modelscope.models import Model\n",
        "# 在参数中指定了双句的两个key\n",
        "preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base', first_sequence='sent1', second_sequence='sent2')\n",
        "model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')\n",
        "data = preprocessor({'sent1': '这件商品很好', 'sent2': '这件商品很优秀'})\n",
        "print(data)\n",
        "print(model(**data)) # AttentionTextClassificationModelOutput(logits=tensor([[-1.3232,  1.5160]], grad_fn=\u003cAddmmBackward0\u003e), loss=None, attentions=None, hidden_states=None)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[注：] 模型可以通过snapshot_download函数下载到本地，也可以是任何一个本地模型目录（代替model_dir即可）。\n",
        "\n",
        "在上面的例子中，SequenceClassificationPreprocessor这个预处理器做了两件事情：\n",
        "1. 从一个完整的输入中分离出句子1和句子2\n",
        "2. 将句子1和句子2传入内部的tokenizer，并生成可以输入模型的数据格式\n",
        "\n",
        "在不同模型和任务下，预处理器的表现不同，请根据您感兴趣的模态来查看不同部分的文档。\n",
        "\n",
        "\n",
        "## 在训练中使用预处理器\n",
        "在训练中使用预处理器的方式请查看[模型的训练](./模型的训练Train.ipynb)。\n",
        "\n",
        "## 在推理中使用预处理器\n",
        "在推理中使用预处理器的方式请查看[模型的推理](./模型的推理Pipeline.ipynb)。\n",
        "\n",
        "## 预处理在文件中的配置\n",
        "为模型配置预处理器请查看[模型的配置](../开发者使用指南/Configuration详解.ipynb)。\n",
        "\n",
        "# NLP\n",
        "\n",
        "NLP模态的预处理器都继承于一个基类[NLPBasePreprocessor]()：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "class NLPBasePreprocessor(Preprocessor):\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_dir: str,\n",
        "                 first_sequence=None,\n",
        "                 second_sequence=None,\n",
        "                 label=None,\n",
        "                 label2id=None,\n",
        "                 mode=ModeKeys.INFERENCE,\n",
        "                 **kwargs):\n",
        "    pass\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "这个类在from_pretrained或构造方法中支持传入：\n",
        "- first_sequence\n",
        "- second_sequence\n",
        "- label\n",
        "这三个字段用来表示数据集中字段1、字段2、label字段的key值\n",
        "- label2id 类型为dict，表示数据集的label和id的映射关系，如果不传入会使用配置文件中的值，如果传入会覆盖配置文件的值。\n",
        "\n",
        "通常来说，这四个参数都和数据集相关，因此用户在使用**任意**NLP预处理器时都可以通过传入这些参数来适配特定数据集。\n",
        "\n",
        "- mode 预处理器的运行状态是推理(inference)、训练（train）还是校验(eval)。\n",
        "\n",
        "其他非标准参数在configuration.json或对应子类的构造方法中都有默认值，一般情况下用户可以不用关心。\n",
        "如果在使用特定模型时需要改动这些参数，您可以查看以下文档：\n",
        "\n",
        "\n",
        "### SequenceClassificationPreprocessor\n",
        "\n",
        "这个预处理器基于transformers.tokenizer实现，用于各输入格式符合transformers标准定义的文本分类预处理。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.hub.snapshot_download import snapshot_download\n",
        "from modelscope.preprocessors import SequenceClassificationPreprocessor\n",
        "\n",
        "model_id = 'damo/nlp_structbert_sentiment-classification_chinese-tiny'\n",
        "model_dir = snapshot_download(model_id)\n",
        "sentence = '启动的时候很大声音，然后就会听到1.2秒的卡察的声音，类似齿轮摩擦的声音'\n",
        "\n",
        "tokenizer = SequenceClassificationPreprocessor(model_dir)\n",
        "result = tokenizer(sentence)\n",
        "print (result)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "分类任务在推理阶段返回的结果如下示：\n",
        "```json\n",
        "{'input_ids': tensor([[ 101, 1423, 1220, 4638, 3198,  952, 2523, 1920, 1898, 7509, 8024, 4197,\n",
        "         1400, 2218,  833, 1420, 1168,  122,  119,  123, 4907, 4638, 1305, 2175,\n",
        "         4638, 1898, 7509, 8024, 5102,  849, 7976, 6762, 3040, 3092, 4638, 1898,\n",
        "         7509,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
        "```\n",
        "\n",
        "这个预处理器的输入参数除支持了NLPBasePreprocessor的参数外，还支持：\n",
        "\n",
        "- sequence_length 输入字符串的最大长度\n",
        "- kwargs transformers的tokenizer各参数均可以通过kwargs传入，运行时会传到tokenizer的__call__方法中\n",
        "\n",
        "在模型的推理阶段，预处理器的mode被设置为`inference`，这时预处理器的返回值类型是Mapping{str: torch.Tensor}，这时用户可以直接将其输入模型中，\n",
        "\n",
        "在训练或校验阶段，预处理器的mode是`train`或`eval`，预处理器的返回值类型是Mapping{str: List}，这时候需要trainer的collate来将其转换为Tensor再输入模型。\n",
        "\n",
        "设置预处理器的mode可以通过Preprocessor.from_pretrained方法的preprocessor_mode参数来实现。在训练阶段，如果用户使用了ModelScope的trainer，那么trainer会自动设置该字段。\n",
        "\n",
        "### TextGenerationPreprocessor\n",
        "\n",
        "这个预处理模块与文本分类任务类似，不同的是由于生成任务主要是一个auto regression的过程而不关心输入token的类型，因此默认输出没有 `token_type_ids`这个字段。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.preprocessors import TextGenerationPreprocessor\n",
        "from modelscope.hub.snapshot_download import snapshot_download\n",
        "\n",
        "model_id = 'damo/nlp_gpt3_text-generation_chinese-base'\n",
        "model_dir = snapshot_download(model_id)\n",
        "sentence = '我很好奇'\n",
        "\n",
        "tokenizer = TextGenerationPreprocessor(model_dir)\n",
        "result = tokenizer(sentence)\n",
        "print(result)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "值得注意的是在训练阶段，输入不仅需要传入原始文本，还要传入续写生成的目标文本，分别用`src_txt`, `tgt_txt`的key，通过字典形式传入预处理方法，相应示例如下：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.preprocessors import TextGenerationPreprocessor\n",
        "from modelscope.utils.constant import ModeKeys\n",
        "from modelscope.hub.snapshot_download import snapshot_download\n",
        "\n",
        "model_dir = snapshot_download('damo/nlp_gpt3_text-generation_chinese-base')\n",
        "src_txt = '我很好奇'\n",
        "tgt_txt = '这个问题是如何被大众解决的'\n",
        "\n",
        "\n",
        "tokenizer = TextGenerationPreprocessor(model_dir, mode=ModeKeys.TRAIN)\n",
        "result = tokenizer({'src_txt':src_txt, 'tgt_txt':tgt_txt})\n",
        "print(result)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 序列标注任务预处理\n",
        "ModelScope序列标注针对的如中文命名实体识别，中文词性标注等任务，文本预处理与分类类似， 具体示例如下：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.preprocessors import TokenClassificationPreprocessor\n",
        "from modelscope.hub.snapshot_download import snapshot_download\n",
        "\n",
        "model_dir = snapshot_download('damo/nlp_gpt3_text-generation_chinese-base')\n",
        "sentence = '今天天气不错，适合出去游玩'\n",
        "\n",
        "tokenizer = TokenClassificationPreprocessor(model_dir)\n",
        "result = tokenizer(sentence)\n",
        "print(result)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "输出结果为：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "‘token':{'input_ids': tensor([[ 101,  791, 1921, 1921, 3698,  679, 7231, 8024, 6844, 1394, 1139, 1343,\n",
        "         3952, 4381,  102]]), \n",
        "        'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), \n",
        "        'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), \n",
        "        'text': '今天天气不错，适合出去游玩'\n",
        "        }\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "在预处理内部，会尝试将句子拆分为基本字母或文字后再进行tokenize动作，以下伪代码说明了这一点：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "sentence = '我是一个机器人'\n",
        "sentence = sentence.replace(' ', '').strip()\n",
        "input_tokens = [t for t in sentence] # ['我', '是', '一', '个', '机', '器', '人']\n",
        "output = tokenize(input_tokens, is_split_into_words=True)\n",
        "...\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "如果在推理时有其他需求导致token处理与此不同，用户可以更改输入为dict并将sentence类型改变为token-list，这样Preprocessor就不会对句子进行二次切分，而会直接输入tokenizer：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.preprocessors import TokenClassificationPreprocessor\n",
        "from modelscope.hub.snapshot_download import snapshot_download\n",
        "\n",
        "model_dir = snapshot_download('damo/nlp_gpt3_text-generation_chinese-base')\n",
        "# A fake example\n",
        "sentence = {'first_sequence': ['今天', '天气', '不错，适合出去游玩']}\n",
        "\n",
        "tokenizer = TokenClassificationPreprocessor(model_dir, first_sequence='first_sequence')\n",
        "result = tokenizer(sentence)\n",
        "print(result)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "训练阶段TokenClassificationPreprocessor只接受dict输入类型，用户可以在config文件中指定dict key，如：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "cfg.preprocessor.first_sequence = 'sentence1'\n",
        "cfg.preprocessor.label = 'label'\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "或者也可以像推理时创建好Preprocessor并传入trainer：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.preprocessors import TokenClassificationPreprocessor\n",
        "\n",
        "tokenizer = TokenClassificationPreprocessor(model_dir, \n",
        "                                            first_sequence='sentence1',\n",
        "                                            label='label')\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "在训练的文件中，请保证first_sequence指代的内容是一个List，其中的每个元素是需要被token化的一个或多个字符，label指代的内容同样是List，其中的每个元素是first_sequence中元素对应的label。\n",
        "如果用户在训练分词任务，而源数据是空格分开的句子，且没有标注好的label，则可以方便地使用另一个Preprocessor来处理这个数据集：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.preprocessors.nlp import WordSegmentationBlankSetToLabelPreprocessor\n",
        "\n",
        "preprocessor = WordSegmentationBlankSetToLabelPreprocessor()\n",
        "preprocessor('今天 天气 不错，适合 出去 游玩')\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 完形填空任务预处理\n",
        "ModelScope也提供了中英文文本Fill Mask完形填空的任务预处理模块，具体使用细节与分类任务类似。具体调用示例如下：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.preprocessors import NLPPreprocessor\n",
        "from modelscope.hub.snapshot_download import snapshot_download\n",
        "\n",
        "model_dir = snapshot_download('damo/nlp_gpt3_text-generation_chinese-base')\n",
        "sentence = '段誉轻[MASK]折扇，摇了摇[MASK]，[MASK]道：“你师父是你的[MASK][MASK]，你师父可不是[MASK]的师父。你师父差得动你，你师父可[MASK]不动我。'\n",
        "\n",
        "tokenizer = NLPPreprocessor(model_dir)\n",
        "result = tokenizer(sentence)\n",
        "print(result)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "输出结果为：\n",
        "```json\n",
        "token: {'input_ids': tensor([[ 101, 3667, 6289, 6768,  103, 2835, 2794, 8024, 3031,  749, 3031,  103,\n",
        "         8024,  103, 6887, 8038,  100,  872, 2360, 4266, 3221,  872, 4638,  103,\n",
        "          103, 8024,  872, 2360, 4266, 1377,  679, 3221,  103, 4638, 2360, 4266,\n",
        "          511,  872, 2360, 4266, 2345, 2533, 1220,  872, 8024,  872, 2360, 4266,\n",
        "         1377,  103,  679, 1220, 2769,  511,  102,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "            0,    0,    0,    0,    0,    0,    0,    0]]), \n",
        "        'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0]]), \n",
        "        'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "       }\n",
        "```\n",
        "\n",
        "### 调用huggingface tokenizer预处理\n",
        "ModelScope作为一个开放的开源平台，在提供自身完整的一套模型处理框架之外，也提供了其他开源框架接入的支持。例如用户可以直接使用huggingface 的tokenizer兼容的方法接口，调用方法如下：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.preprocessors import build_preprocessor\n",
        "from modelscope.utils.constant import Fields,InputFields\n",
        "\n",
        "sentence = 'Do not meddle in the affairs of wizards, for they are subtle and quick to anger.'\n",
        "\n",
        "cfg = dict(type='Tokenize', tokenizer_name='bert-base-cased')\n",
        "preprocessor = build_preprocessor(cfg, Fields.nlp)\n",
        "input = {InputFields.text: sentence}\n",
        "result = preprocessor(input)\n",
        "print(result)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "返回结果为：\n",
        "```json\n",
        "'token':{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, \n",
        "                       1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, \n",
        "                       4248, 2000, 4963, 1012, 102], \n",
        "         'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
        "                            0, 0, 0, 0, 0, 0, 0], \n",
        "         'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "                            1, 1, 1, 1, 1, 1, 1]\n",
        "        }\n",
        "```\n",
        "\n",
        "## 图像预处理\n",
        "### 图像分割任务预处理\n",
        "ModelScope提供了一个较为通用的图像分割预处理函数，名称为`image-instance-segmentation-preprocessor`，目前支持通过在配置文件中传入该preprocessor，trainer在build阶段就会自动加载该preprocessor，并根据当前Model状态自动切换为`train`或`eval`所对应的预处理。\n",
        "目前`image-instance-segmentation-preprocessor`提供了常用的分割图像预处理方法，包括`Resize`，`RandomFlip`，`Normalize`，`Pad`等，此处各处理函数借鉴了.ipynbetetcion。下面示例展示了具体preprocessor的配置，其中type指定preprocessor类型为`image-instance-segmentation-preprocessor`，`train`字段表示模型训练时所对应的预处理，`val`字段表示模型推理时所对应的预处理：\n",
        "```json\n",
        "\"preprocessor\": {\n",
        "    \"type\": \"image-instance-segmentation-preprocessor\",\n",
        "    \"train\": [\n",
        "        {\n",
        "            \"type\": \"LoadImageFromFile\"\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"LoadAnnotations\",\n",
        "            \"with_bbox\": true,\n",
        "            \"with_mask\": true\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Resize\",\n",
        "            \"img_scale\": [\n",
        "                [666, 320],\n",
        "                [666, 400]\n",
        "            ],\n",
        "            \"multiscale_mode\": \"range\",\n",
        "            \"keep_ratio\": true\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"RandomFlip\",\n",
        "            \"flip_ratio\": 0.5\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Normalize\",\n",
        "            \"mean\": [123.675, 116.28, 103.53],\n",
        "            \"std\": [58.395, 57.12, 57.375],\n",
        "            \"to_rgb\": true\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Pad\",\n",
        "            \"size_divisor\": 32\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"DefaultFormatBundle\"\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Collect\",\n",
        "            \"keys\": [\"img\", \"gt_bboxes\", \"gt_labels\", \"gt_masks\"],\n",
        "            \"meta_keys\": [\n",
        "                \"filename\", \"ori_filename\", \"ori_shape\",\n",
        "                \"img_shape\", \"pad_shape\", \"scale_factor\", \"flip\",\n",
        "                \"flip_direction\", \"img_norm_cfg\", \"ann_file\",\n",
        "                \"classes\"\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    \"val\": [\n",
        "        {\n",
        "            \"type\": \"LoadImageFromFile\"\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Resize\",\n",
        "            \"img_scale\": [1333, 800],\n",
        "            \"keep_ratio\": true\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"RandomFlip\",\n",
        "            \"flip_ratio\": 0.0\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Normalize\",\n",
        "            \"mean\": [123.675, 116.28, 103.53],\n",
        "            \"std\": [58.395, 57.12, 57.375],\n",
        "            \"to_rgb\": true\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Pad\",\n",
        "            \"size_divisor\": 32\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"ImageToTensor\",\n",
        "            \"keys\": [\"img\"]\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"Collect\",\n",
        "            \"keys\": [\"img\"],\n",
        "            \"meta_keys\": [\n",
        "                \"filename\", \"ori_filename\", \"ori_shape\",\n",
        "                \"img_shape\", \"pad_shape\", \"scale_factor\", \"flip\",\n",
        "                \"flip_direction\", \"img_norm_cfg\", \"ann_file\",\n",
        "                \"classes\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "},\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
