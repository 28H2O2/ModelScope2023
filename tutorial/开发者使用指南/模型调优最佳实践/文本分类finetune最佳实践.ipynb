{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "本文详细介绍文本分类任务finetune的流程，以及基础配置和高级配置的使用。\n",
        "## Train的调用流程\n",
        "### 载入数据\n",
        "ModelScope可以提供了标准的`MsDataset`接口供用户进行基于ModelScope生态的数据源加载，也支持来自第三方库用户自定义数据集加载，如NLP领域的`Datasets`库。\n",
        "具体示例如下，如何加载NLP领域里面的clue榜单的afqmc（Ant Financial Question Matching Corpus）数据集：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# Option 1: 使用ModelScope dataset-hub上存储的原生的MsDataset\n",
        "from modelscope.msdatasets import MsDataset \n",
        "dataset = MsDataset.load('clue', subset_name='afqmc', split='train')\n",
        "\n",
        "# Option 2: 通过MsDataset加载huggingface上的数据集\n",
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.utils.constant import Hubs\n",
        "\n",
        "dataset = MsDataset.load('clue', subset_name='afqmc', split='train', hub = Hubs.huggingface)\n",
        "\n",
        "# Option 3: 直接使用huggingface的api加载huggingface数据集\n",
        "# 可参考huggingface/Datasets说明：https://huggingface.co/docs/datasets/index \n",
        "\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "具体MsDataset 使用可以参考接口文档： [数据的处理](../../数据集/数据集介绍.ipynb); \n",
        "\n",
        "### 数据预处理\n",
        "相同任务的训练和推理的数据预处理，可以采用相同的Preprocessor。\n",
        "在配置文件中传入注册的 preprocessor名称即可，trainer在build阶段会自动加载相应preprocessor，并根据当前Mode置于 `traine`或者`eval `的状态。如下面示例代码，在进行NLP情感分类下游任务`sentiment-classification`的finetune过程，需要调用的preprocessor在configuration.json配置与推理阶段相同，会通过type = 'sen-cls-tokenizer' 构建出对应的前处理模块。\n",
        "同时，在处理dataset过程中如何申明dataset中label的字段、样本名称字段和样本种类等信息，均需要添加到配置文件的的train.dataset字段中。\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"task\": \"sentiment-classification\",\n",
        "  \"preprocessor\":{\n",
        "    \"type\": \"sen-cls-tokenizer\",\n",
        "  },\n",
        "  \"model\": {\n",
        "      \"type\": \"structbert\",\n",
        "  },\n",
        "  \"framework\": \"pytorch\",\n",
        "  \"train\": {\n",
        "      \"dataset\": {\n",
        "        \"train\": {\n",
        "          \"labels\": [\"0\", \"1\"],\n",
        "          \"first_sequence\": \"sentence1\",\n",
        "          \"second_sequence\": \"sentence2\",\n",
        "          \"label\": \"label\",\n",
        "        }\n",
        "      }, \n",
        "      ...\n",
        "  },\n",
        "  \"evaluation\": {\n",
        "     ...\n",
        "  },    \n",
        "}\n",
        "\n",
        "```\n",
        "具体的预处理方法可在对应注册为sen-cls-tokenizer的 前处理模块进行开发，相应使用方法参考 Preprocessor接口文档：[数据的预处理](../../ModelScope%20Library教程/数据的预处理.ipynb)\n",
        "### 训练\n",
        "由trainer相关的接口文档可以了解到，训练过程核心流程由dataset、dataloader、optimizer、lr_scheduler和hooks等组件功能组成，具体是通过在configuration.json配置文件中申明的方式注册进入trainer的流程中，具体参考：[configuration详解](../Configuration详解.ipynb)\n",
        "#### 基础配置\n",
        "在训练开始前需要配置好相应的trainer配置文件， 下面给一个完整的NLP进行情感分类下游任务finetune的配置。\n",
        "用户在实际使用过程中，如果示例无法提供帮助，可以根据自己实际训练要求，针对optimizer/lr_scheduler/hooks进行定制注册，并在配置文件中通过type字段申明相应定制方法进行使用。\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"task\": \"sentiment-classification\",\n",
        "  \"preprocessor\":{\n",
        "    \"type\": \"sen-cls-tokenizer\",\n",
        "  },\n",
        "  \"model\": {\n",
        "      \"type\": \"structbert\",\n",
        "  },\n",
        "  \"framework\": \"pytorch\",\n",
        "  \"train\": {\n",
        "      \"work_dir\": \"/tmp\",\n",
        "      \"max_epochs\": 5,\n",
        "      \"dataset\": {\n",
        "          \"train\": {\n",
        "              \"labels\": [\"0\", \"1\"],\n",
        "              \"first_sequence\": \"sentence1\",\n",
        "              \"second_sequence\": \"sentence2\",\n",
        "              \"label\": \"label\",\n",
        "          },\n",
        "      },\n",
        "      \"dataloader\": {\n",
        "          \"batch_size_per_gpu\": 32,\n",
        "          \"workers_per_gpu\": 1\n",
        "      },\n",
        "      \"optimizer\": {\n",
        "          \"type\": \"AdamW\",\n",
        "          \"lr\": 2e-5,\n",
        "          \"options\": {}\n",
        "      },\n",
        "      \"lr_scheduler\": {\n",
        "          \"type\": \"LinearLR\",\n",
        "          \"start_factor\": 1.0,\n",
        "          \"end_factor\": 0.0,\n",
        "          \"total_iters\": null,\n",
        "          \"options\": { \n",
        "              \"by_epoch\": false\n",
        "          }\n",
        "      },\n",
        "      \"hooks\": [{\n",
        "          \"type\": \"CheckpointHook\",\n",
        "          \"interval\": 1\n",
        "      }, {\n",
        "          \"type\": \"TextLoggerHook\",\n",
        "          \"interval\": 1\n",
        "      }, {\n",
        "          \"type\": \"IterTimerHook\"\n",
        "      }, {\n",
        "          \"type\": \"EvaluationHook\",\n",
        "          \"by_epoch\": false,\n",
        "          \"interval\": 100\n",
        "      }]\n",
        "  },\n",
        "  \"evaluation\": {\n",
        "      \"dataloader\": {\n",
        "          \"batch_size_per_gpu\": 32,\n",
        "          \"workers_per_gpu\": 1,\n",
        "          \"shuffle\": false\n",
        "      }\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "综合上述内容，可通过如下代码进行模型的finetune训练\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.trainers import build_trainer\n",
        "\n",
        "workspace = \"/worksapce\"\n",
        "\n",
        "# 如果数据集在MsDataset中存在，推荐使用MsDataset.load方法\n",
        "train_dataset = MsDataset.load('afqmc_small', split='train')\n",
        "eval_dataset = MsDataset.load('afqmc_small', split='validation')\n",
        "\n",
        "# 也可以读取huggingface hub上存储的数据\n",
        "# train_dataset = MsDataset.load('afqmc_small', split='train', hub = Hubs.huggingface)\n",
        "# eval_dataset = MsDataset.load('afqmc_small', split='validation', hub = Hubs.huggingface)\n",
        "\n",
        "model_id = 'damo/nlp_structbert_sentence-similarity_chinese-base'\n",
        "kwargs = dict(\n",
        "    model=model_id,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    work_dir=workspace)\n",
        "\n",
        "trainer = build_trainer(default_args=kwargs)\n",
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 高级配置\n",
        "在实际过程用户可能会频繁对配置进行调整，不光是训练相关的参数，很可能对应的下游任务都会变化，因此我们提供了高级配置方式供算法用户使用，从而减少不必要的configuration文件改写。\n",
        "具体包括自定义cfg文件覆盖，以及自定义cfg_function的方法进行。\n",
        "\n",
        "- 自定义cfg file， 通过覆盖更新cfg_file文件进行代码内的配置调整\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "import os\n",
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.trainers import build_trainer\n",
        "from modelscope.utils.hub import read_config\n",
        "\n",
        "workspace = \"/worksapce\"\n",
        "\n",
        " # 如果数据集在MsDataset中存在，推荐使用MsDataset.load方法\n",
        "train_dataset = MsDataset.load('afqmc_small', split='train')\n",
        "eval_dataset = MsDataset.load('afqmc_small', split='validation')\n",
        "\n",
        "# 也可以读取huggingface hub上存储的数据\n",
        "# train_dataset = MsDataset.load('afqmc_small', split='train', hub = Hubs.huggingface)\n",
        "# eval_dataset = MsDataset.load('afqmc_small', split='validation', hub = Hubs.huggingface)\n",
        "\n",
        "model_id = 'damo/nlp_structbert_sentence-similarity_chinese-base'\n",
        "cfg = read_config(model_id)\n",
        "cfg.train.max_epochs = 20\n",
        "cfg.train.work_dir = tmp_dir\n",
        "cfg_file = os.path.join(tmp_dir, 'config.json')\n",
        "cfg.dump(cfg_file)\n",
        "kwargs = dict(\n",
        "    model=model_id,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    work_dir=workspace,\n",
        "    cfg_file=cfg_file,\n",
        ")\n",
        "\n",
        "trainer = build_trainer(default_args=kwargs)\n",
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- NLP模型可以使用自定义cfg_function申明，通过代码内自定义相应内容进行配置更新\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "import os\n",
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.trainers import build_trainer\n",
        "from modelscope.utils.hub import read_config\n",
        "\n",
        "workspace = \"/worksapce\"\n",
        "\n",
        " # 如果数据集在MsDataset中存在，推荐使用MsDataset.load方法\n",
        "train_dataset = MsDataset.load('afqmc_small', split='train')\n",
        "eval_dataset = MsDataset.load('afqmc_small', split='validation')\n",
        "\n",
        "def cfg_modify_fn(cfg):\n",
        "    cfg.task = 'sentence-similarity'\n",
        "    cfg['preprocessor'] = {'type': 'sen-sim-tokenizer'}\n",
        "    cfg.train.optimizer.lr = 2e-5\n",
        "    cfg['dataset'] = {\n",
        "        'train': {\n",
        "            'labels': ['0', '1'],\n",
        "            'first_sequence': 'sentence1',\n",
        "            'second_sequence': 'sentence2',\n",
        "            'label': 'label',\n",
        "        }\n",
        "    }\n",
        "    cfg.train.max_epochs = 10\n",
        "    cfg.train.lr_scheduler = {\n",
        "        'type': 'LinearLR',\n",
        "        'start_factor': 1.0,\n",
        "        'end_factor': 0.0,\n",
        "        'total_iters':\n",
        "        int(len(train_dataset) / 32) * cfg.train.max_epochs,\n",
        "        'options': {\n",
        "            'by_epoch': False\n",
        "        }\n",
        "    }\n",
        "    cfg.train.hooks = [{\n",
        "        'type': 'CheckpointHook',\n",
        "        'interval': 1\n",
        "    }, {\n",
        "        'type': 'TextLoggerHook',\n",
        "        'interval': 1\n",
        "    }, {\n",
        "        'type': 'IterTimerHook'\n",
        "    }, {\n",
        "        'type': 'EvaluationHook',\n",
        "        'by_epoch': False,\n",
        "        'interval': 100\n",
        "    }]\n",
        "    return cfg\n",
        "\n",
        "# 也可以读取huggingface hub上存储的数据\n",
        "# train_dataset = MsDataset.load('afqmc_small', split='train', hub = Hubs.huggingface)\n",
        "# eval_dataset = MsDataset.load('afqmc_small', split='validation', hub = Hubs.huggingface)\n",
        "\n",
        "model_id = 'damo/nlp_structbert_backbone_tiny_std'\n",
        "kwargs = dict(\n",
        "    model=model_id,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    work_dir=workspace,\n",
        "    cfg_modify_fn=cfg_modify_fn,\n",
        ")\n",
        "\n",
        "trainer = build_trainer(name='nlp-base-trainer', default_args=kwargs)\n",
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 评估\n",
        "#### 交叉验证\n",
        "交叉验证是在train时同步进行的，基于在配置文件中的 train.hooks的 EvaluationHook，具体配置如下：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "{\n",
        "   ...\n",
        "  \"train\": {\n",
        "     ...\n",
        "      \"hooks\": [\n",
        "          ...\n",
        "          , {\n",
        "          \"type\": \"EvaluationHook\",\n",
        "          \"by_epoch\": false,\n",
        "          \"interval\": 100\n",
        "      }]\n",
        "  },\n",
        "\n",
        "}\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "用户可以根据自己实际情况进行调整，也可自行注册相应hook，并通过type字段注册在配置文件中进行调用。\n",
        "#### 训练后验证\n",
        "\n",
        "1. 指定并加载验证数据集\n",
        "2. build_trainer\n",
        "3. 调用evaluate方法\n",
        "\n",
        "如下代码展示了模型的验证流程\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.trainers import build_trainer\n",
        "\n",
        "# 如果数据集在MsDataset中存在，推荐使用MsDataset.load方法\n",
        "train_dataset = MsDataset.load('afqmc_small', split='train')\n",
        "eval_dataset = MsDataset.load('afqmc_small', split='validation')\n",
        "\n",
        "# 也可以读取huggingface hub上存储的数据\n",
        "# train_dataset = MsDataset.load('afqmc_small', split='train', hub = Hubs.huggingface)\n",
        "# eval_dataset = MsDataset.load('afqmc_small', split='validation', hub = Hubs.huggingface)\n",
        "\n",
        "kwargs = dict(\n",
        "    model='damo/nlp_structbert_sentence-similarity_chinese-base',\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    work_dir='/workspace')\n",
        "\n",
        "trainer = build_trainer(default_args=kwargs)\n",
        "# 直接调用trainer.evaluate，可以传入train阶段生成的ckpt\n",
        "# 也可以不传入参数，直接验证model\n",
        "metrics = trainer.evaluate(checkpoint_path=None)\n",
        "print(metrics)\n",
        "\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 指标\n",
        "指标用来衡量某个具体任务的验证结果，用户可以查看如下package找到已支持指标类的列表：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "import modelscope.metrics\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "以NLP领域举例，目前支持的指标有以下几种：\n",
        "\n",
        "- SequenceClassificationMetric\n",
        "   - 指标名称为seq-cls-metric\n",
        "   - 为分类任务提供，返回的指标为accuracy\n",
        "- TokenClassificationMetric\n",
        "   - 指标名称为token-cls-metric\n",
        "   - 为序列标注任务提供，返回的指标为F1/recall/precision/accuracy\n",
        "   - 支持提供每个子类别的指标，通过return_entity_level_metrics=True开启\n",
        "- TextGenerationMetric\n",
        "   - 指标名称为text-gen-metric\n",
        "   - 为生成任务提供，返回的指标为根据rouge计算的F1\n",
        "\n",
        "代码中我们为支持finetune的各任务类型指定了默认metric类，：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.metrics.builder import task_default_metrics\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "用户也可以在cfg中指定自己需要的指标类型：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# 指定使用SequenceClassificationMetric\n",
        "cfg.evaluation.metrics = 'seq-cls-metric'\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "指标的具体介绍可以参考：[模型的评估](../../ModelScope%20Library教程/模型的评估.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
