{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 什么是模型导出\n",
        "\n",
        "当我们使用深度学习模型进行实际生产过程中的服务时，很多时候不会直接使用原始的模型文件（比如，PyTorch的pytorch_model.bin等需要通过load_state_dict加载的二进制文件），\n",
        "这是因为这些模型文件只能在python环境中运行，而且无法进行\"优化\"。举个例子，假如某个环境是嵌入式设备的单片机，那么在它的上面安装python环境是非常费力的；另外如果模型比较大，想要\n",
        "达到比较高的QPS，模型需要摆脱python环境而使用更快的C++库，或进行一些硬件相关的优化，或进行一些算子的融合。\n",
        "\n",
        "在这种情况下，使用原始的代码和二进制文件运行就比较得不偿失，因此各类\n",
        "算法库都提供了对应的\"导出格式\"，比如PyTorch的TorchScript，tensorflow的GraphDef，或者跨框架格式ONNX。这些格式不仅包含了模型的各类参数，也包含了模型动态图本身，因此可以脱离\n",
        "python环境独立运行并可以获得一定的运行加速，不少算子库也支持以这些格式为起点进行后续优化。ModelScope提供了其内模型的导出方法，用户可以自由选用。\n",
        "\n",
        "# 导出为ONNX格式\n",
        "\n",
        "[ONNX](https://onnx.ai/) 全称为开放神经网络交换格式（Open Neural Network Exchange），是微软和Facebook（Meta）联合提出用于表示深度学习模型的文件格式。\n",
        "其特点为标准的文件格式，且具备平台无关性。也就是说，用户在任意框架（TensorFlow/PyTorch/JAX等）中训练得到的原始模型\n",
        "都可以转换为这种格式进行存储和优化，或转换为其他框架专用的模型文件。ONNX文件和其他输出格式一样，不仅存储了模型权重，\n",
        "也存储了模型DAG图以及一些有用的辅助信息。\n",
        "\n",
        "如果您的生产环境使用ONNX，或需要ONNX格式进行后续优化，您可以使用ModelScope提供的ONNX转换工具进行模型导出。\n",
        "\n",
        "## 导出方法\n",
        "\n",
        "首先我们需要初始化一个已支持Exporter模块的模型：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.models import Model\n",
        "model_id = 'damo/nlp_structbert_sentence-similarity_chinese-base'\n",
        "model = Model.from_pretrained(model_id)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "下面我们就可以将其导出为对应格式：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.exporters import Exporter\n",
        "# shape参数用来生成dummy inputs\n",
        "# 在NLP领域中一般len(shape) == 2, 分别代表batch_size和sequence_length\n",
        "output_files = Exporter.from_model(model).export_onnx(shape=(2, 256), opset=13, output_dir='/tmp')\n",
        "print(output_files) # {'model': '/tmp/model.onnx'}\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "opset是onnx算子版本，具体可以参考[这里](https://onnxruntime.ai/docs/reference/compatibility.html)。\n",
        "\n",
        "在导出完成后，ModelScope会使用dummy_inputs验证onnx文件的正确性，因此如果导出过程不报错就证明导出过程已经成功了。\n",
        "\n",
        "需要注意的是，验证过程需要onnx包和onnxruntime包，如果您的环境中没有安装，会看到如下报错：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "\"modelscope - WARNING - Cannot validate the exported onnx file, because the installation of onnx or onnxruntime cannot be found\"\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "如果需要验证过程可以安装这两个包：\n",
        "```shell\n",
        "pip install onnx\n",
        "pip install onnxruntime\n",
        "```\n",
        "或使用conda命令安装：\n",
        "```shell\n",
        "conda install -c conda-forge onnx\n",
        "conda install -c conda-forge onnxruntime\n",
        "```\n",
        "\n",
        "如果需要在GPU环境下进行验证过程，可以改为使用下面的命令：\n",
        "```shell\n",
        "pip install onnx\n",
        "pip install onnxruntime-gpu\n",
        "```\n",
        "onnxruntime-gpu和CUDA版本和cuDNN版本强相关，安装时请注意版本对应。具体可以参考[这里](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html)。\n",
        "\n",
        "## 如何在外部模型上使用导出功能\n",
        "\n",
        "如果目前支持导出的模型中没有您需要的模型，或该模型是一个torch.nn.Module，可以手动传入dummy_inputs，inputs和outputs来实现。\n",
        "\n",
        "如下展示了导出transformers库模型的示例。首先把模型和tokenizer初始化出来：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "之后我们使用tokenizer来生成dummy_inputs并调用导出工具：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.exporters import TorchModelExporter\n",
        "from collections import OrderedDict\n",
        "# 假设最大支持256长度的句子\n",
        "dummy_inputs = tokenizer(tokenizer.unk_token, padding='max_length', max_length=256, return_tensors='pt')\n",
        "dynamic_axis = {0: 'batch', 1: 'sequence'}\n",
        "inputs = OrderedDict([\n",
        "    ('input_ids', dynamic_axis),\n",
        "    ('attention_mask', dynamic_axis),\n",
        "    ('token_type_ids', dynamic_axis),\n",
        "])\n",
        "outputs = OrderedDict({'logits': {0: 'batch'}})\n",
        "output_files = TorchModelExporter().export_onnx(model=model, dummy_inputs=dummy_inputs, inputs=inputs, \n",
        "                                                outputs=outputs, output_dir='/tmp')\n",
        "print(output_files) # {'model': '/tmp/model.onnx'}\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "inputs和outputs参数用来指示动态dimension，其格式为OrderedDict，key为输入/输出参数名，\n",
        "value为格式如{0: 'batch', 1: 'sequence'}的动态dimension序号和名称。例子中的0代表tensor第一个维度，1代表tensor第二个维度，\n",
        "batch/sequence为自定义的维度名称。\n",
        "\n",
        "在实现上，ModelScope调用了torch.onnx.export方法用以导出onnx，这个方法的输入实际上需要一个ScriptModule，但是也兼容\n",
        "torch.nn.Module。如果传入模型不是ScriptModule，export方法内部会使用trace方式将模型转为ScriptModule。由于模型结构的复杂性，\n",
        "大部分ModelScope模型尚不支持script方式进行模型导出，这一部分我们仍在探索中。\n",
        "有关trace和script方式的使用可以参考[官方文档](https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting)。\n",
        "\n",
        "\n",
        "## 支持导出ONNX的ModelScope模型\n",
        "\n",
        "\n",
        "| 模型         |                       任务 |\n",
        "|------------|-------------------------:|\n",
        "| structbert |                      nli |\n",
        "| structbert | sentiment-classification |\n",
        "| structbert |      sentence-similarity |\n",
        "| structbert | zero-shot-classification |\n",
        "\n",
        "注意，这里指的支持导出是Exporter中存在对应某一模型的具体实现，用户仍然可以使用上述的外部模型导出手动定制自己的导出过程。\n",
        "\n",
        "\n",
        "## 如何使用ONNX模型\n",
        "\n",
        "首先需要安装onnxruntime运行时环境，onnxruntime支持多种语言多个平台，具体可以参考[这里](https://onnxruntime.ai/)。\n",
        "\n",
        "为简便演示，我们在这里展示了python环境中onnxruntime的使用方法，模型为上面外部模型导出的onnx文件，onnxruntime安装过程可以参考上面的文档。\n",
        "\n",
        "首先构造inputs：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "dummy_inputs = tokenizer('这是一个测试的例子', padding='max_length', max_length=256, return_tensors='np')\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "调用onnxruntime来运行模型：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "import onnxruntime as ort\n",
        "ort_session = ort.InferenceSession('/tmp/model.onnx')\n",
        "outputs = ort_session.run(['logits'], dict(dummy_inputs))\n",
        "print(outputs)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 导出为TorchScript格式\n",
        "\n",
        "同ONNX类似，[TorchScript](https://pytorch.org/docs/master/jit.html)也是深度学习模型的中间表示格式，不同的是它是基于PyTorch框架的。\n",
        "Torch模型通过导出变为TorchScript格式后，就可以脱离python环境运行或进行后续的推理加速。\n",
        "\n",
        "ModelScope也提供了模型转为TorchScript的能力。\n",
        "\n",
        "## 导出方法\n",
        "\n",
        "首先我们需要初始化一个已支持导出TorchScript的模型：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.models import Model\n",
        "model_id = 'damo/nlp_structbert_sentence-similarity_chinese-base'\n",
        "model = Model.from_pretrained(model_id)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "下面我们就可以将其导出为对应格式：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# 由于TorchScript是Pytorch特有的格式，因此需要使用TorchModelExporter\n",
        "from modelscope.exporters import TorchModelExporter\n",
        "# shape参数用来生成dummy inputs\n",
        "# 在NLP领域中一般len(shape) == 2, 分别代表batch_size和sequence_length\n",
        "output_files = TorchModelExporter.from_model(model).export_torch_script(shape=(2, 256), output_dir='/tmp')\n",
        "print(output_files) # {'model': '/tmp/model.ts'}\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "模型转换TorchScript有两种方式，Script和Trace。Script方式对以加载完毕的模型代码进行静态分析，并生成TorchScript文件。\n",
        "而Trace方式仍然需要一个dummy input用来追溯模型的动态图，用来后续分析生成。\n",
        "\n",
        "Script方式的优点是，可以将源代码的特性包含进去，比如if分支条件等。但由于使用了AST方式进行代码分析，其对模型的要求也较高，比如\n",
        "需要模型在输入参数上有类型标注，方法中没有无法追溯的动态类型等。Trace方式要求较低，只需要一个构造好的dummy input既可根据动态图\n",
        "生成静态图。但trace方式要求输入全部为tensor，且模型逻辑中不包含tensor无参与的if分支条件，也给导出带来了一定限制。\n",
        "\n",
        "ModelScope模型大部分都支持trace方式，因此我们把trace方式选择为默认的导出方式。\n",
        "\n",
        "同样地，在导出完成后，ModelScope会使用dummy_inputs验证ts文件的正确性，因此如果导出过程不报错就证明导出过程已经成功了。\n",
        "\n",
        "注意：Script方式生成的文件不支持动态尺寸输入，也就是说，用于以后生产环境中的输入tensor尺寸必须和dummy inputs相同。如果实际输入尺寸小于\n",
        "dummy input尺寸，请注意在数据预处理过程中添加padding。\n",
        "\n",
        "\n",
        "## 如何在外部模型上使用导出功能\n",
        "\n",
        "如果目前支持导出的模型中没有您需要的模型，或该模型是一个torch.nn.Module，可以手动传入dummy_inputs来实现。\n",
        "\n",
        "如下展示了导出transformers库模型的示例。首先把模型和tokenizer初始化出来：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "之后我们使用tokenizer来生成dummy_inputs并调用导出工具：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.exporters import TorchModelExporter\n",
        "from collections import OrderedDict\n",
        "# 假设最大支持256长度的句子\n",
        "dummy_inputs = tokenizer(tokenizer.unk_token, padding='max_length', max_length=256, return_tensors='pt')\n",
        "output_files = TorchModelExporter().export_torch_script(model=model, dummy_inputs=dummy_inputs, output_dir='/tmp')\n",
        "print(output_files) # {'model': '/tmp/model.ts'}\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 支持导出TorchScript的modelscope模型\n",
        "\n",
        "\n",
        "| 模型         |                       任务 |\n",
        "|------------|-------------------------:|\n",
        "| structbert |                      nli |\n",
        "| structbert | sentiment-classification |\n",
        "| structbert |      sentence-similarity |\n",
        "| structbert | zero-shot-classification |\n",
        "\n",
        "注意，这里指的支持导出是Exporter中存在对应某一模型的具体实现，用户仍然可以使用上述的外部模型导出手动定制自己的导出过程。\n",
        "\n",
        "\n",
        "## 如何使用TorchScript模型\n",
        "\n",
        "TorchScript模型支持多种语言环境，有关使用可以参考[这里](https://pytorch.org/tutorials/advanced/cpp_export.html#step-3-loading-your-script-module-in-c)。\n",
        "\n",
        "为简便演示，我们在这里展示了python环境中TorchScript的使用方法，模型为上面外部模型导出的ts文件。\n",
        "\n",
        "首先构造inputs：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "dummy_inputs = tokenizer('这是一个测试的例子', padding='max_length', max_length=256, return_tensors='pt')\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "调用torch来运行模型：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "import torch\n",
        "ts_model = torch.jit.load('/tmp/model.ts')\n",
        "ts_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = ts_model.forward(**dummy_inputs)\n",
        "print(outputs)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
